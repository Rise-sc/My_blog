---
title: 正则化
date: 2025-09-3 15:00:00
updated: 2025-08-27 15:00:00
tags:
  - 机器学习
categories:
  - 数学
description: 正则化
---
# 正则化

正则化（Regularization）是一种在机器学习中用来**防止过拟合**的技术。

正则化通过在模型的损失函数中增加一个惩罚项来实现, 这个惩罚项会根据模型参数(权重)的大小来增加损失

##  1. L1, L2正则化

###  1.1 L1正则化

L1 正则化的惩罚项是所有模型**权重（W）的绝对值之和**。
$$
loss(W,b) = \sum_{i=1}^{m}( \hat{y_i} - y_i)^2 + \lambda \sum_{j=1}^{n} |w_j|
$$

- λ (lambda) 是一个超参数，它控制惩罚的强度。λ 越大，惩罚越重，模型越简单。

- ∣wi∣ 是第 i 个权重的绝对值。

L1 正则化最大的特点就是能产生一个**稀疏**（sparse）模型，即大部分权重都会被压缩到**0**。

- **自动特征选择**：当一个特征的权重变为0时，它对模型的预测就没有影响了。因此，L1 正则化可以自动地从大量特征中挑选出那些最重要的，而忽略不重要的。这使得模型更精简，也更易于理解。

- **几何解释**：在二维空间中，L1 正则化的约束区域是一个**菱形**。损失函数（圆形）的最优解和菱形约束的交点，通常会发生在菱形的尖角上，而这些尖角正好位于坐标轴上（即某个权重为0）。



###  1.2 L2正则化

L2 正则化的惩罚项是所有模型**权重（W）的平方和**。
$$
loss(W,b) = \sum_{i=1}^{m}( \hat{y_i} - y_i)^2 + \lambda \sum_{j=1}^{n} w_j^2
$$

- λ 和 L1 一样，控制惩罚的强度。

- wi2 是第 i 个权重的平方。

L2 的特点：权重收缩, L2 正则化倾向于将所有权重都压缩到**一个非常接近0的小值**，但很少会将它们完全变为0

- **权重收缩**：L2 正则化不会像 L1 那样直接进行特征选择，它只是让所有不重要的权重变得非常小，从而防止任何单个特征对模型产生过大的影响，使得模型更加平滑。

- **几何解释**：L2 正则化的约束区域是一个**圆形**。损失函数（圆形）和圆形约束的交点，通常不会落在坐标轴上。

###  对比

|     特性     |                      L1 正则化 (Lasso)                       |           L2 正则化 (Ridge)            |
| :----------: | :----------------------------------------------------------: | :------------------------------------: |
|  **惩罚项**  |                        权重绝对值之和                        |               权重平方和               |
|   **结果**   |                 **稀疏模型**（部分权重为0）                  |    **非稀疏模型**（所有权重都缩小）    |
| **主要效果** |                       **自动特征选择**                       |        **权重收缩，防止过拟合**        |
| **应用场景** | 当你希望模型简单，且需要进行**特征选择**时（比如有大量不相关的特征） | 当你希望模型平滑，且所有特征都很重要时 |
|   **缺点**   |          产生的模型可能不稳定，对数据的微小变化敏感          | 不会产生稀疏性，模型的可解释性相对较差 |

##  2. Dropout

Dropout 是神经网络中的一种**正则化**技术，旨在防止模型过拟合。它的名字本身就说明了它的工作原理：“dropout”就是“丢弃”的意思。

### 2.1 Dropout 的工作原理

想象一个神经网络，它有很多层和很多神经元。在训练过程中，每一次迭代，**Dropout 都会随机地“关闭”一些神经元**，就像它们暂时不存在了一样。这些被关闭的神经元在这一次的前向传播和反向传播中都不会被使用。

下次迭代时，**Dropout 会重新随机选择另一组神经元进行关闭**。这个过程在整个训练过程中反复进行。

### 2.2 为什么 Dropout 能防止过拟合？

Dropout 的工作原理有点像**“集体决策”**。它迫使神经网络中的每个神经元不能依赖于其他任何特定的神经元。这有几个好处：

1. **减少神经元之间的依赖**：如果两个神经元总是同时出现并一起工作，它们可能会形成一种“共适应”（co-adaptation）。Dropout 打破了这种依赖，迫使每个神经元学习更独立、更有用的特征。
2. **强制学习鲁棒的特征**：当一些神经元被随机丢弃时，剩下的神经元必须在没有它们的帮助下完成任务。这迫使网络学习那些更“鲁棒”和更具有泛化能力的特征，而不是依赖于某些特定的“捷径”或“旁门左道”。
3. **近似集成学习**：每次训练迭代时，由于不同的神经元被丢弃，网络实际上都在训练一个“微小”的、略有不同的子网络。在测试时，所有神经元都处于激活状态，这相当于对所有这些不同的子网络进行“投票”，从而得到一个更稳定、更准确的结果。这和集成学习（ensemble learning）的思想非常相似。

**总结**：Dropout 通过在训练过程中随机丢弃神经元，打破了神经元之间的共依赖性，迫使网络学习更具泛化性的特征，从而有效防止了过拟合，提高了模型的鲁棒性。